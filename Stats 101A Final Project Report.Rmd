---
title: "Stats 101A - Final Project Report"
subtitle: "Predicting House Prices in Ames, Iowa"
author: "Hao Ma"
date: "Winter 2019"
output: pdf_document
---

## **Abstract**
This is a report of the regression model study on Kaggle for predicting house prices in Ames, Iowa. The study consists of analyzing the given data set, selecting predictors, creating new factor variables, choosing appropriate transformations and interactions, building the final regression model, and checking the validity of the model. 
The user name on Kaggle is 'Hao Ma Lec 1' with public ranking #109 and a $R^2$ of 0.90562; private ranking #94 and a $R^2$ of 0.91281. There are 13 predictors in the final model with a $R^2$ of 0.907.


## **Introduction**
Given 80 variables related to houses in Ames, Iowa, this study builds a multiple linear regression model using the training data set to predict the price of houses in the testing data set. The training data set contains 2500 observations of houses with 80 variables (excluding the numbers of observation and including the sale prices). The goal is to build the model and apply it to the testing data set to get predictions of sale prices as close to the real prices as possible.


## **Methodology**
```{r echo=FALSE, message=FALSE}
library(corrplot)
library(ggplot2)
library(alr3)
```

```{r}
train <- read.csv("HTrainW19Final.csv")

ggplot(data = train, aes(SalePrice)) + geom_histogram(aes(y = ..density..), binwidth = 5000, 
                                                      color = "grey30", fill = "white") + 
  geom_density(alpha = .2, fill = "antiquewhite3") + 
stat_function(fun = dnorm, args = list(mean = mean(train$SalePrice), sd = sd(train$SalePrice)), 
              col = "red")
```
Firstly, get a basic idea of what the distribution of sale prices looks like. It is clear that transformation is needed to make the response variable more 'normal'.

```{r}
numeric <- train[,sapply(train, is.numeric)]
corr <- round(cor(numeric, use = "complete.obs"), 4)
corrplot(corr, method = "square", tl.cex = 0.7)
```
Separate numeric variables from the data set and based on the correlation plot, 11 numeric variables are chosen as predictors: `LotArea`, `OverallQual`, `TotalBsmtSF`, `YearBuilt`, `YearRemodAdd`, `GrLivArea`, `FullBath`, `TotRmsAbvGrd`, `Fireplaces`, `GarageCars`, `MasVnrArea`.

```{r}
sum(is.na(train$LotArea))
sum(is.na(train$OverallQual))
sum(is.na(train$TotalBsmtSF))
sum(is.na(train$YearBuilt))
sum(is.na(train$YearRemodAdd))
sum(is.na(train$GrLivArea))
sum(is.na(train$FullBath))
sum(is.na(train$TotRmsAbvGrd))
sum(is.na(train$Fireplaces))
sum(is.na(train$GarageCars))
sum(is.na(train$MasVnrArea)) 

train$TotalBsmtSF[which(is.na(train$TotalBsmtSF))] <-
  median(na.omit(train$TotalBsmtSF[which(train$TotalBsmtSF != 0)]))
train$MasVnrArea[which(is.na(train$MasVnrArea))] <- 
  median(na.omit(train$MasVnrArea[which(train$MasVnrArea != 0)])) 
```
It is important to see if there are any NA's in the selected variables. In this study, there are NA's in `TotalBsmtSF` and `MasVnrArea`. They are replaced by the median (median without 0's and NA's).

```{r}
train$Age1 <- 2019 - train$YearBuilt
train$Age2 <- 2019 - train$YearRemodAdd
```
Two new variables are created to replace YearBuilt and YearRemodAdd: `Age1` and `Age2`, since a year such as 1990 does not make sense in a regression formula. The actual number of years from that year to now is a good choice.

```{r}
par(mfrow=c(2,2))
plot(train$SalePrice ~ train$FullBath)
plot(train$SalePrice ~ train$Fireplaces)
plot(train$SalePrice ~ train$GarageCars)
plot(train$SalePrice ~ train$MasVnrArea)
summary(train$MasVnrArea)

train$FullBath <- as.factor(train$FullBath)
train$Fireplaces <- as.factor(train$Fireplaces)
train$GarageCars <- as.factor(train$GarageCars)

med.mas.vnr <- median(train$MasVnrArea[which(train$MasVnrArea != 0)])
for(i in 1:nrow(train)){
  if(train$MasVnrArea[i] == 0) {train$MasVnr[i] <- "No MasVnr"} 
  if(train$MasVnrArea[i] != 0 & train$MasVnrArea[i] <= med.mas.vnr) {train$MasVnr[i] <- "Small MasVnr"} 
  if(train$MasVnrArea[i] != 0 & train$MasVnrArea[i] > med.mas.vnr) {train$MasVnr[i] <- "Large MasVnr"}
}
train$MasVnr <- as.factor(train$MasVnr)
```
After analyzing some of the selected predictors, it is clear that turning `FullBath`, `Fireplaces`, and `GarageCars` into factor variables would be a good choice since they represent the number of full bathrooms, fireplaces, and cars in garage, therefore they all have several categories and it would be better to use them as categorical predictors rather than numerical predictors. 
In addition, there are too many 0's in the variable `MasVnrArea`, and even its median turns out to be 0. Thus considering grouping the numerical predictor `MasVnrArea` to create a new categorical (factor) predictor `MasVnr` with 3 categories: 'No MasVnr', 'Small', and 'Large'. 

```{r}
summary(lm(SalePrice ~ Neighborhood, data = train))
plot(train$SalePrice ~ train$Neighborhood)
summary(lm(SalePrice ~ Foundation, data = train))
plot(train$SalePrice ~ train$Foundation)

sum(is.na(train$Neighborhood))
sum(is.na(train$Foundation))
```
Summary (focusing on $R^2$ values) and plots are analyzed to select other categorical predictors. In this study, `Neighborhood` and `Foundation` are chosen since they have relatively high $R^2$ values which means that they could explain a lot of variations in the response variable (see also the box plots). In addition, they have no NA values, which is another important reason for them to be chosen.

```{r warning=FALSE}
model1 <- lm(SalePrice ~ LotArea + OverallQual + TotalBsmtSF + Age1 + Age2 + GrLivArea + 
               FullBath + TotRmsAbvGrd + Fireplaces + Neighborhood + Foundation + GarageCars +
               MasVnr, data = train)      
summary(model1)
par(mfrow=c(2,2))
plot(model1)
vif(model1)
```
This is the first model without any transformations or interaction terms. Clearly, transformations are needed and the predictors also needs some modifications to improve the model and the $R^2$ value.

```{r warning=FALSE}
summary(powerTransform(cbind(train$SalePrice, train$LotArea, train$GrLivArea)~1))

tSalePrice <- log(train$SalePrice)
tLotArea <- log(train$LotArea)
tGrLivArea <- log(train$GrLivArea)
```
Using box-cox method, the suggested transformation for `SalePrice`, `LotArea`, and `GrLivArea` is log transformation. Note that the other numerical predictors are not transformed because they have 0's.

```{r warning=FALSE}
model2_1 <- lm(tSalePrice ~ tLotArea + OverallQual + TotalBsmtSF + Age1 + Age2 + 
               tGrLivArea + FullBath + TotRmsAbvGrd + Fireplaces + Neighborhood + 
               Foundation + GarageCars + MasVnr, data = train)
summary(model2_1)
par(mfrow=c(2,2))
plot(model2_1)
vif(model2_1)
```
This is the second model, with transformations added. This model has higher $R^2$ and better diagnostics. But it seems that grouping the variable `Neighborhood` would be essential to avoid high vif and to fix the problem of too many insignificant slopes for `Neighborhood`.

```{r}
plot(train$SalePrice ~ train$Neighborhood)

N <- as.integer(train$Neighborhood)
for(i in 1:nrow(train)){
   if(N[i] %in% c(1,5,6,7,9,12,13,17,20,21)){
     train$Ngroup[i] <- "Group 1"
   }else if(N[i] %in% c(2,3,11,15)){
     train$Ngroup[i] <- "Group 2"
   }else if(N[i] %in% c(4,8,10,18,19,23)){
     train$Ngroup[i] <- "Group 3"
   }else{
     train$Ngroup[i] <- "Group 4"
   }
}

train$Ngroup <- as.factor(train$Ngroup)

table(train$Ngroup)
plot(train$SalePrice~as.factor(train$Ngroup))
summary(lm(train$SalePrice~train$Ngroup))
```
After examining the first box plot (`SalePrice` vs `Neighborhood`), the neighborhoods are divided into 4 groups based on the distribution of sale prices. Group 1: 'Blmngtn', 'ClearCr', 'CollgCr', 'Crawfor', 'Gilbert', 'Mitchel', 'NAmes', 'NWAmes', 'SawyerW', 'Somerst'; Group 2: 'Blueste', 'BrDale', 'MeadoV', 'NpkVill'; Group 3: 'BrkSide', 'Edwards', 'IDOTRR', 'OldTown', 'Sawyer', 'SWISU'; Group 4: 'NoRidge', 'NridgHt', 'StoneBr', 'Timber', 'Veenker'.  
As we can see, the new factor variable `Ngroup` itself explains a lot of variations in the response variable.

```{r warning=FALSE}
model2_2 <- lm(tSalePrice ~ tLotArea + OverallQual + TotalBsmtSF + Age1 + Age2 + 
               tGrLivArea + FullBath + TotRmsAbvGrd + Fireplaces + Ngroup + 
               Foundation + GarageCars + MasVnr, data = train)
summary(model2_2)
par(mfrow=c(2,2))
plot(model2_2)
vif(model2_2)
```
The model looks more valid, but the $R^2$ value still needs to be higher, therefore consider adding interaction terms to build the final model.

```{r warning=FALSE}
model3 <- lm(tSalePrice ~ MasVnr:tLotArea + MasVnr:OverallQual + MasVnr:TotalBsmtSF + 
               MasVnr:Age1 + MasVnr:Age2 + MasVnr:tGrLivArea + FullBath + 
               MasVnr:TotRmsAbvGrd + Fireplaces + MasVnr:Ngroup + Foundation + 
               MasVnr:GarageCars, data = train)
summary(model3)
par(mfrow=c(2,2))
plot(model3)

check <- lm(tSalePrice ~ tLotArea + OverallQual + TotalBsmtSF + Age1 + Age2 + tGrLivArea + 
              FullBath + TotRmsAbvGrd + Fireplaces + Ngroup + Foundation + GarageCars + 
              MasVnr, data = train)
vif(check)
```
In this final step, each predictor is taken out once and added as an interaction term with the other predictors. Finally, the optimal interaction term is chosen as `MasVnr`: the (valid) model gives the highest adjusted $R^2$ with `MasVnr` interacting with the other predictors, excluding those variables with NA's appearing in the summary of model (here NA shows serious multicollinearity problem). Then we get the final model with adjusted $R^2$ 0.907, as mentioned at the very beginning.
**Evidence for the validity of the final model:** 
Residuals vs Fitted plot and Scale Location plot both give an almost flat line which means that there are no problems with linearity, independence, or constant variance of errors. The Residuals vs Leverage plot shows one bad leverage point (#1308) which acceptable given the large data set. Although there are some points deviating from the straight line in Normal QQ plot, this is still acceptable given the large data set. Moreover, there is no multicollinearity problem since the vif's are all very small (smaller than 5).    


## **Results**
```{r}
test <- read.csv("HTestW19Final No Y values.csv")

test$Age1 <- 2019 - test$YearBuilt
test$Age2 <- 2019 - test$YearRemodAdd

test$TotalBsmtSF[which(is.na(test$TotalBsmtSF))] <- 
  median(na.omit(test$TotalBsmtSF[which(test$TotalBsmtSF != 0)]))

tLotArea <- log(test$LotArea)
tGrLivArea <- log(test$GrLivArea)

test$GarageCars <- as.factor(test$GarageCars)
test$Fireplaces <- as.factor(test$Fireplaces)
test$FullBath <- as.factor(test$FullBath)

test$MasVnrArea[which(is.na(test$MasVnrArea))] <- 
  median(na.omit(test$MasVnrArea[which(test$MasVnrArea != 0)]))
med.mas.vnr <- median(test$MasVnrArea[which(test$MasVnrArea != 0)])
for(i in 1:nrow(test)){
  if(test$MasVnrArea[i] == 0) {test$MasVnr[i] <- "No MasVnr"} 
  if(test$MasVnrArea[i] != 0 & test$MasVnrArea[i] <= med.mas.vnr) {test$MasVnr[i] <- "Small MasVnr"} 
  if(test$MasVnrArea[i] != 0 & test$MasVnrArea[i] > med.mas.vnr) {test$MasVnr[i] <- "Large MasVnr"}
}
test$MasVnr <- as.factor(test$MasVnr)


Ntest <- as.integer(test$Neighborhood)
for(i in 1:nrow(test)){
   if(Ntest[i] %in% c(1,5,6,7,9,12,13,17,20,21)){
     test$Ngroup[i] <- "Group 1"
   }else if(Ntest[i] %in% c(2,3,11,15)){
     test$Ngroup[i] <- "Group 2"
   }else if(Ntest[i] %in% c(4,8,10,18,19,23)){
     test$Ngroup[i] <- "Group 3"
   }else{
     test$Ngroup[i] <- "Group 4"
   }
}
test$Ngroup <- as.factor(test$Ngroup)


p <- predict(model3, newdata = test)
p[is.na(p)] <- median(na.omit(p)) 
price <- exp(p)
my_prediction <- data.frame(Ob = 1:1500, SalePrice = round(price,2))
write.csv(my_prediction, "SalePrice_Hao_Ma_Lec1.csv")

head(round(price,2))
summary(round(price,2))
```
For the testing data set, work is done to create the same new variables as in the training data as well as performing same transformation and modifications, in order to apply the model to testing data set. The predictions of house prices for the testing dataset using the final model built in this study are written into a csv file. The first several entries and the summary statistics of the predictions are shown above.


## **Discussion**
The CI, t-test are still reasonable and the estimates of slopes are still unbiased even if the Normal QQ plot does not look very good, regarding our large data set; variable selecting methods based on adjusted $R^2$, AIC, AICc, and BIC are not used in this study; there should be a more advanced method for finding the best interaction term to save time. 


## **Limitations and Conclusions**
As mentioned above, the Normal QQ plot does not look very good, therefore PI's might be questionable. Also, there is for sure a bad leverage point based on Residuals vs Leverage plot. Some more advanced analysis or models are needed to improve the predictions. 
Overall, we can conclude that the multiple linear regression model is valid and acceptable; the predictions and inference based on the model are valid and acceptable; the minor problems are also acceptable.


## **References**
Training, testing data sets and data descriptions from Kaggle: https://www.kaggle.com/c/stat101ahouseprice/data
